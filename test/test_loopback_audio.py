#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WASAPI LoopbackéŸ³é¢‘æµ‹è¯•ç¨‹åº
ç”¨äºå½•åˆ¶ç³»ç»ŸéŸ³é¢‘å¹¶è¿›è¡Œå¯è§†åŒ–åˆ†æ
"""

import pyaudiowpatch as pyaudio
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import threading
import time
import queue
import sys
from datetime import datetime

class LoopbackAudioTester:
    def __init__(self):
        self.CHUNK = 1024
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1  # è¾“å‡ºå•å£°é“
        self.RATE = 48000  # ä½¿ç”¨è®¾å¤‡é»˜è®¤é‡‡æ ·ç‡
        self.RECORD_SECONDS = 10
        
        self.audio = pyaudio.PyAudio()
        self.stream = None
        self.audio_queue = queue.Queue()
        self.recording = False
        
        # ç”¨äºå­˜å‚¨éŸ³é¢‘æ•°æ®
        self.audio_data = []
        self.time_data = []
        
    def find_loopback_device(self):
        """æŸ¥æ‰¾WASAPI Loopbackè®¾å¤‡"""
        print("\n=== æ‰«æWASAPI Loopbackè®¾å¤‡ ===")
        
        loopback_devices = []
        device_count = self.audio.get_device_count()
        
        for i in range(device_count):
            try:
                device_info = self.audio.get_device_info_by_index(i)
                if device_info.get('name', '').find('Loopback') != -1:
                    loopback_devices.append((i, device_info))
                    print(f"è®¾å¤‡ {i}: {device_info['name']}")
                    print(f"  é€šé“æ•°: {device_info['maxInputChannels']}")
                    print(f"  é‡‡æ ·ç‡: {device_info['defaultSampleRate']}Hz")
                    print()
            except Exception as e:
                continue
                
        if not loopback_devices:
            print("âŒ æœªæ‰¾åˆ°WASAPI Loopbackè®¾å¤‡")
            return None
            
        # ä¼˜å…ˆé€‰æ‹©HECATEå£°å¡è®¾å¤‡
        hecate_device = None
        for device_index, device_info in loopback_devices:
            if 'HECATE' in device_info['name'].upper():
                hecate_device = (device_index, device_info)
                break
        
        if hecate_device:
            device_index, device_info = hecate_device
            print(f"âœ… é€‰æ‹©HECATEè®¾å¤‡: {device_info['name']}")
        else:
            # å¦‚æœæ²¡æœ‰HECATEè®¾å¤‡ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„loopbackè®¾å¤‡
            device_index, device_info = loopback_devices[0]
            print(f"âœ… é€‰æ‹©è®¾å¤‡: {device_info['name']}")
        
        # ä½¿ç”¨è®¾å¤‡çš„é»˜è®¤é‡‡æ ·ç‡
        self.RATE = int(device_info['defaultSampleRate'])
        print(f"ğŸ“Š ä½¿ç”¨é‡‡æ ·ç‡: {self.RATE}Hz")
        
        return device_index, device_info
    
    def analyze_channel_data(self, data, channels):
        """åˆ†æå¤šé€šé“éŸ³é¢‘æ•°æ®"""
        if channels == 1:
            return data
            
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_array = np.frombuffer(data, dtype=np.int16)
        
        if len(audio_array) < self.CHUNK * channels:
            return np.zeros(self.CHUNK, dtype=np.int16)
            
        # é‡æ–°æ•´å½¢ä¸º(samples, channels)
        try:
            reshaped = audio_array[:self.CHUNK * channels].reshape(self.CHUNK, channels)
            
            # åˆ†æå„é€šé“ç‰¹å¾
            print(f"\n--- é€šé“åˆ†æ (æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}) ---")
            channel_stats = []
            for ch in range(min(channels, 8)):
                ch_data = reshaped[:, ch]
                ch_std = np.std(ch_data)
                ch_max = np.max(np.abs(ch_data))
                ch_rms = np.sqrt(np.mean(ch_data.astype(np.float32)**2))
                channel_stats.append({
                    'channel': ch,
                    'std': ch_std,
                    'max': ch_max,
                    'rms': ch_rms,
                    'data': ch_data
                })
                print(f"CH{ch}: std={ch_std:.1f}, max={ch_max}, rms={ch_rms:.1f}")
            
            # ä½¿ç”¨æ”¹è¿›çš„downmixç®—æ³•
            if channels == 2:
                # ç«‹ä½“å£°
                mono_data = ((reshaped[:, 0].astype(np.float32) + reshaped[:, 1].astype(np.float32)) / 2).astype(np.int16)
            elif channels <= 8:
                # å¤šé€šé“ - ä¸»è¦ä½¿ç”¨å‰ä¸¤ä¸ªé€šé“
                left = reshaped[:, 0].astype(np.float32)
                right = reshaped[:, 1].astype(np.float32) if channels > 1 else left
                
                # æ£€æŸ¥å…¶ä»–é€šé“æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                other_channels = np.zeros_like(left)
                active_other_channels = 0
                
                if channels > 2:
                    for ch in range(2, min(channels, 8)):
                        ch_data = reshaped[:, ch].astype(np.float32)
                        # æ£€æŸ¥é€šé“æ˜¯å¦æœ‰æœ‰æ•ˆä¸”ä¸åŒçš„æ•°æ®
                        if (np.std(ch_data) > 100 and 
                            not np.allclose(ch_data, left, atol=1000) and 
                            not np.allclose(ch_data, right, atol=1000)):
                            other_channels += ch_data * 0.1
                            active_other_channels += 1
                            print(f"  -> CH{ch} åŒ…å«æœ‰æ•ˆæ•°æ®ï¼Œå·²æ··å…¥")
                
                print(f"  -> æ´»è·ƒçš„å…¶ä»–é€šé“æ•°: {active_other_channels}")
                
                # ä¸»è¦åŸºäºç«‹ä½“å£°ï¼Œå°‘é‡æ··å…¥å…¶ä»–é€šé“
                mono_data = ((left + right) / 2 + other_channels * 0.2).astype(np.int16)
                mono_data = np.clip(mono_data, -32768, 32767)
            else:
                # è¶…è¿‡8é€šé“ï¼Œåªä½¿ç”¨å‰ä¸¤ä¸ª
                left = reshaped[:, 0].astype(np.float32)
                right = reshaped[:, 1].astype(np.float32) if channels > 1 else left
                mono_data = ((left + right) / 2).astype(np.int16)
            
            return mono_data
            
        except Exception as e:
            print(f"âŒ é€šé“åˆ†æé”™è¯¯: {e}")
            return np.zeros(self.CHUNK, dtype=np.int16)
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°"""
        if status:
            print(f"éŸ³é¢‘çŠ¶æ€: {status}")
            
        # å°†éŸ³é¢‘æ•°æ®æ”¾å…¥é˜Ÿåˆ—
        self.audio_queue.put(in_data)
        return (None, pyaudio.paContinue)
    
    def start_recording(self, device_index, device_info):
        """å¼€å§‹å½•åˆ¶éŸ³é¢‘"""
        try:
            # è·å–è®¾å¤‡çš„å®é™…é€šé“æ•°
            device_channels = int(device_info['maxInputChannels'])
            print(f"\nğŸ™ï¸ å¼€å§‹å½•åˆ¶éŸ³é¢‘...")
            print(f"è®¾å¤‡é€šé“æ•°: {device_channels}")
            print(f"è¾“å‡ºé€šé“æ•°: {self.CHANNELS}")
            print(f"é‡‡æ ·ç‡: {self.RATE}Hz")
            print(f"å½•åˆ¶æ—¶é•¿: {self.RECORD_SECONDS}ç§’")
            
            # åˆ›å»ºéŸ³é¢‘æµ
            self.stream = self.audio.open(
                format=self.FORMAT,
                channels=device_channels,  # ä½¿ç”¨è®¾å¤‡çš„å®é™…é€šé“æ•°
                rate=self.RATE,
                input=True,
                input_device_index=device_index,
                frames_per_buffer=self.CHUNK,
                stream_callback=self.audio_callback
            )
            
            self.recording = True
            self.stream.start_stream()
            
            # å½•åˆ¶éŸ³é¢‘æ•°æ®
            start_time = time.time()
            frame_count = 0
            
            while self.recording and (time.time() - start_time) < self.RECORD_SECONDS:
                try:
                    # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘æ•°æ®
                    data = self.audio_queue.get(timeout=1.0)
                    
                    # åˆ†æå¹¶è½¬æ¢éŸ³é¢‘æ•°æ®
                    mono_data = self.analyze_channel_data(data, device_channels)
                    
                    # å­˜å‚¨éŸ³é¢‘æ•°æ®ç”¨äºåˆ†æ
                    self.audio_data.extend(mono_data)
                    current_time = time.time() - start_time
                    self.time_data.extend([current_time + i/self.RATE for i in range(len(mono_data))])
                    
                    frame_count += 1
                    if frame_count % 20 == 0:  # æ¯20å¸§æ‰“å°ä¸€æ¬¡çŠ¶æ€
                        rms = np.sqrt(np.mean(mono_data.astype(np.float32)**2))
                        print(f"å¸§ {frame_count}: RMS={rms:.1f}, æœ€å¤§å€¼={np.max(np.abs(mono_data))}")
                        
                except queue.Empty:
                    print("âš ï¸ éŸ³é¢‘é˜Ÿåˆ—è¶…æ—¶")
                    continue
                except Exception as e:
                    print(f"âŒ å¤„ç†éŸ³é¢‘æ•°æ®é”™è¯¯: {e}")
                    break
            
            print(f"\nâœ… å½•åˆ¶å®Œæˆï¼Œå…±å½•åˆ¶ {len(self.audio_data)} ä¸ªé‡‡æ ·ç‚¹")
            
        except Exception as e:
            print(f"âŒ å½•åˆ¶é”™è¯¯: {e}")
        finally:
            self.stop_recording()
    
    def stop_recording(self):
        """åœæ­¢å½•åˆ¶"""
        self.recording = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        
    def analyze_audio(self):
        """åˆ†æå½•åˆ¶çš„éŸ³é¢‘æ•°æ®"""
        if not self.audio_data:
            print("âŒ æ²¡æœ‰éŸ³é¢‘æ•°æ®å¯åˆ†æ")
            return
            
        audio_array = np.array(self.audio_data, dtype=np.float32)
        time_array = np.array(self.time_data)
        
        print(f"\n=== éŸ³é¢‘åˆ†æç»“æœ ===")
        print(f"æ€»é‡‡æ ·ç‚¹æ•°: {len(audio_array)}")
        print(f"å½•åˆ¶æ—¶é•¿: {time_array[-1]:.2f}ç§’")
        print(f"RMSå€¼: {np.sqrt(np.mean(audio_array**2)):.2f}")
        print(f"æœ€å¤§å€¼: {np.max(np.abs(audio_array)):.2f}")
        print(f"æ ‡å‡†å·®: {np.std(audio_array):.2f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„éŸ³é¢‘ä¿¡å·
        rms_threshold = 100  # RMSé˜ˆå€¼
        max_threshold = 1000  # æœ€å¤§å€¼é˜ˆå€¼
        
        rms_value = np.sqrt(np.mean(audio_array**2))
        max_value = np.max(np.abs(audio_array))
        
        if rms_value > rms_threshold and max_value > max_threshold:
            print("âœ… æ£€æµ‹åˆ°æœ‰æ•ˆéŸ³é¢‘ä¿¡å·")
        else:
            print("âš ï¸ éŸ³é¢‘ä¿¡å·è¾ƒå¼±æˆ–å¯èƒ½å­˜åœ¨é—®é¢˜")
            
        # æ£€æŸ¥æ˜¯å¦æœ‰ç”µæµå£°ç‰¹å¾ï¼ˆé«˜é¢‘å™ªéŸ³ï¼‰
        # è®¡ç®—é¢‘è°±
        fft = np.fft.fft(audio_array)
        freqs = np.fft.fftfreq(len(audio_array), 1/self.RATE)
        magnitude = np.abs(fft)
        
        # æ£€æŸ¥é«˜é¢‘éƒ¨åˆ†çš„èƒ½é‡
        high_freq_mask = freqs > 8000  # 8kHzä»¥ä¸Š
        high_freq_energy = np.mean(magnitude[high_freq_mask])
        total_energy = np.mean(magnitude)
        
        high_freq_ratio = high_freq_energy / total_energy if total_energy > 0 else 0
        
        print(f"é«˜é¢‘èƒ½é‡æ¯”ä¾‹: {high_freq_ratio:.3f}")
        if high_freq_ratio > 0.3:
            print("âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„é«˜é¢‘å™ªéŸ³ï¼ˆç”µæµå£°ï¼‰")
        else:
            print("âœ… é«˜é¢‘å™ªéŸ³æ°´å¹³æ­£å¸¸")
    
    def plot_audio(self):
        """ç»˜åˆ¶éŸ³é¢‘æ³¢å½¢å’Œé¢‘è°±å›¾"""
        if not self.audio_data:
            print("âŒ æ²¡æœ‰éŸ³é¢‘æ•°æ®å¯ç»˜åˆ¶")
            return
            
        audio_array = np.array(self.audio_data, dtype=np.float32)
        time_array = np.array(self.time_data)
        
        # åˆ›å»ºå›¾å½¢
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('WASAPI LoopbackéŸ³é¢‘åˆ†æ', fontsize=16)
        
        # 1. å®Œæ•´æ³¢å½¢å›¾
        ax1.plot(time_array, audio_array, 'b-', linewidth=0.5)
        ax1.set_title('å®Œæ•´éŸ³é¢‘æ³¢å½¢')
        ax1.set_xlabel('æ—¶é—´ (ç§’)')
        ax1.set_ylabel('å¹…åº¦')
        ax1.grid(True, alpha=0.3)
        
        # 2. å±€éƒ¨æ³¢å½¢å›¾ï¼ˆå‰1ç§’ï¼‰
        if len(time_array) > 0:
            mask = time_array <= 1.0
            if np.any(mask):
                ax2.plot(time_array[mask], audio_array[mask], 'r-', linewidth=1)
                ax2.set_title('å‰1ç§’æ³¢å½¢è¯¦å›¾')
                ax2.set_xlabel('æ—¶é—´ (ç§’)')
                ax2.set_ylabel('å¹…åº¦')
                ax2.grid(True, alpha=0.3)
        
        # 3. é¢‘è°±å›¾
        fft = np.fft.fft(audio_array)
        freqs = np.fft.fftfreq(len(audio_array), 1/self.RATE)
        magnitude = np.abs(fft)
        
        # åªæ˜¾ç¤ºæ­£é¢‘ç‡éƒ¨åˆ†
        positive_freq_mask = freqs >= 0
        ax3.semilogx(freqs[positive_freq_mask], 20*np.log10(magnitude[positive_freq_mask] + 1e-10), 'g-')
        ax3.set_title('é¢‘è°±å›¾')
        ax3.set_xlabel('é¢‘ç‡ (Hz)')
        ax3.set_ylabel('å¹…åº¦ (dB)')
        ax3.grid(True, alpha=0.3)
        ax3.set_xlim(20, self.RATE//2)
        
        # 4. RMSéšæ—¶é—´å˜åŒ–
        window_size = self.RATE // 10  # 0.1ç§’çª—å£
        rms_values = []
        rms_times = []
        
        for i in range(0, len(audio_array) - window_size, window_size//2):
            window = audio_array[i:i+window_size]
            rms = np.sqrt(np.mean(window**2))
            rms_values.append(rms)
            rms_times.append(time_array[i + window_size//2] if i + window_size//2 < len(time_array) else time_array[-1])
        
        ax4.plot(rms_times, rms_values, 'm-', linewidth=2)
        ax4.set_title('RMSéšæ—¶é—´å˜åŒ–')
        ax4.set_xlabel('æ—¶é—´ (ç§’)')
        ax4.set_ylabel('RMSå€¼')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'loopback_audio_analysis_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"\nğŸ“Š éŸ³é¢‘åˆ†æå›¾å·²ä¿å­˜: {filename}")
        
        plt.show()
    
    def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸµ WASAPI LoopbackéŸ³é¢‘æµ‹è¯•ç¨‹åº")
        print("=" * 50)
        
        try:
            # æŸ¥æ‰¾loopbackè®¾å¤‡
            device_result = self.find_loopback_device()
            if not device_result:
                return
                
            device_index, device_info = device_result
            
            # å¼€å§‹å½•åˆ¶
            self.start_recording(device_index, device_info)
            
            # åˆ†æéŸ³é¢‘
            self.analyze_audio()
            
            # ç»˜åˆ¶å›¾å½¢
            self.plot_audio()
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        except Exception as e:
            print(f"âŒ æµ‹è¯•é”™è¯¯: {e}")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_recording()
        if self.audio:
            self.audio.terminate()
        print("\nğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    tester = LoopbackAudioTester()
    tester.run_test()

if __name__ == "__main__":
    main()